// Utility types

type ExactlyOneOfSome<T, K extends keyof T = keyof T> = K extends unknown
    ? { 
        [I in keyof T]?: I extends K
            ? T[I]
            : never;
    }
    : never;

type ExactlyOne<T> = ExactlyOneOfSome<T, keyof T>;

export type MinMax<T> = {
    min: T;
    max: T;
}

// ElasticSearch Typings
// ElasticSearch Helper Types

export type ScriptLanguage = 'painless' | 'expression' | 'mustache' | 'java';

export type SortOrder = 'asc' | 'desc';

export type SortInstruction<Dataset> = 
    Record<keyof Dataset, SortOrder | { order: SortOrder }>;

export type SortOptions<Dataset> = 
    SortOrder | 
    SortInstruction<Dataset> | 
    SortInstruction<Dataset>[];

export type AggSortOptions = {
    _key?: 'asc' | 'desc';
    _count?: 'asc' | 'desc';
};

export type SourceOptions<Dataset> = string | string[];  //TODO incomplete

export type DateInterval = '1m' | '1h' | '1d' | '1w' | '1M' | '1q' | '1y';

export type Existence = Opaque<object, "existence">;
//Aggregation Helper Types

// This is a rather 'abstract' base interface
export type ScriptBase = {
	lang?: ScriptLanguage;
	params?: Record<string, string | number>;
}

export type LiteralScript = ScriptBase | {
	source: string;
}

export type ScriptReference = ScriptBase | {
	id: string;
}

// These are the actual values accepted by ES.
// Note that ScriptBase is not icluded
export type Script = {
    script: string | LiteralScript | ScriptReference;
}

export type MetricField<Dataset, Key extends keyof Dataset = keyof Dataset> = 
    Key extends keyof Dataset
        ? {
            /**
             * The field that values should be extracted from
             */
            field: Key;

            /**
             * A value to use if the field is missing entirely
             */
            missing?: Dataset[Key];
        }
        : never;

export type MetricAggregationConfig<Dataset, Keys extends keyof Dataset> = 
    MetricField<Dataset, Keys> /* | Script */;
// TODO Investigate why using the `Script` type above makes completions fail

export type MetricAggregations<Dataset, Fields extends keyof Dataset> = {
    /**
     * Computes the average of numeric values that are extracted from the
     * aggregated documents.
     */
    avg: MetricAggregationConfig<Dataset, Fields>;

    /**
     * Computes boxplot of numeric values extracted from the aggregated
     * documents. 
     */
    boxplot: MetricAggregationConfig<Dataset, Fields> & {
        /**
         * Approximate algorithms must balance memory utilization with estimation
         * accuracy. This balance can be controlled using a compression parameter.
         */
        compression?: integer;
    };

    /**
     * Calculates an approximate count of distinct values.
     */
    cardinality: MetricAggregationConfig<Dataset, Fields> & {
        /**
         * Allows to trade memory for accuracy, and defines a unique count below
         * which counts are expected to be close to accurate. Above this value,
         * counts might become a bit more fuzzy. The maximum supported value is
         * 40000, thresholds above this number will have the same effect as a
         * threshold of 40000. The default value is 3000.
         */
        precision_threshold?: integer;
    };

    /**
     * Computes stats over numeric values extracted from the aggregated
     * documents.
     */
    extended_stats: MetricAggregationConfig<Dataset, Fields> & {
        /**
         * sigma can be any non-negative double which controls how many standard
         * deviations +/- from the mean should be displayed.
         */
        sigma?: float;
    };

    /**
     * Computes the bounding box containing all geo values for a field.
     */
    geo_bounds: MetricAggregationConfig<Dataset, Fields> & {
        /**
         * Specifies whether the bounding box should be allowed to overlap the
         * international date line.
         */
        wrap_longitude?: boolean;
    };

    /**
     * Computes the weighted centroid from all coordinate values for geo fields.
     */
    geo_centroid: MetricAggregationConfig<Dataset, Fields>;

    /**
     * Returns the maximum value among the numeric values extracted from the
     * aggregated documents.
     */
    max: MetricAggregationConfig<Dataset, Fields>;

    /**
     * Returns the minimum value among numeric values extracted from the
     * aggregated documents.
     */
    min: MetricAggregationConfig<Dataset, Fields>;

    /**
     * Calculates one or more percentile ranks over numeric values extracted
     * from the aggregated documents.
     */
    percentile_ranks: MetricAggregationConfig<Dataset, Fields> & {
        /**
         * List of treshold values to use for ranking.
         */
        values: float[];
        /**
         * By default, the buckets are returned as an ordered array. It is also
         * possible to request the response as a hash instead keyed by the
         * buckets keys.
         */
        keyed?: boolean;

        /**
         * (High Dynamic Range Histogram) is an alternative implementation that
         * can be useful when calculating percentile ranks for latency
         * measurements as it can be faster than the t-digest implementation
         * with the trade-off of a larger memory footprint.
         */
        hdr?: {
            number_of_significant_value_digits: integer;
        }
    };

    /**
     * Calculates one or more percentiles over numeric values extracted from the
     * aggregated documents. 
     */
    percentiles: MetricAggregationConfig<Dataset, Fields> & {
        /**
         * List of treshold values to use for ranking.
         */
        percents?: float[];
        /**
         * Setting the keyed flag to true will associate a unique string 
         * key with each bucket and return the ranges as a hash rather than 
         * an array.
         */
        keyed?: boolean;
        /** 
         * Format: {compression: integer}: The compression parameter limits
         * the maximum number of nodes to 20 * compression.
         */
        tdigest?: {
            compression: integer;
        };
        /**
         * hdr object indicates that HDR Histogram should be used to calculate
         * the percentiles and specific settings for this algorithm can be
         * specified inside the object. Format: 
         * {number_of_significant_value_digits: integer}
         */
		hdr?: { 
            number_of_significant_value_digits: integer;
        };
    };

    /**
     * A metric aggregation that executes using scripts to provide a metric
     * output.
     */
    scripted_metric: {
        init_script: Script;
        map_script: Script;
        combine_script: Script;
        reduce_script: Script;
    }

    /**
     * Computes stats over numeric values extracted from the aggregated
     * documents.
     */
    stats: MetricAggregationConfig<Dataset, Fields>;

    /**
     * Computes statistics over string values extracted from the aggregated
     * documents.
     */
    string_stats: MetricAggregationConfig<Dataset, Fields> & {
        show_distribution?: boolean;
    };

    /**
     * Sums up numeric values that are extracted from the aggregated documents.
     */
    sum: MetricAggregationConfig<Dataset, Fields>;

    /**
     * Performs a statistical hypothesis test in which the test statistic
     * follows a Studentâ€™s t-distribution under the null hypothesis on numeric 
     * values.
     */
    t_test: {
        /**
         * First field of numeric type.
         */
        a: MetricAggregationConfig<Dataset, Fields> & {
            filter?: {};
        };
        /**
         * Second field of numeric type.
         */
        b: MetricAggregationConfig<Dataset, Fields> & {
            filter?: {};
        };
        /**
         * The type of the test can be specified using the type parameter.
         */
        type: 'paired' | 'homoscedastic' | 'heteroscedastic';
    }

    /**
     * Keeps track of the most relevant document being aggregated. This
     * aggregator is intended to be used as a sub aggregator, so that the top
     * matching documents can be aggregated per bucket.
     */
    top_hits: {
        /**
         * The offset from the first result you want to fetch.
         */
        from?: integer;

        /**
         * The maximum number of top matching hits to return per bucket. By
         * default the top three matching hits are returned.
         */
        size?: integer;

        /**
         * How the top matching hits should be sorted. By default the hits are
         * sorted by the score of the main query.
         */
        sort?: SortOptions<Dataset>;

        /**
         * Field names
         */
		_source?: SourceOptions<Dataset>;
    };

    /**
     * Selects metrics from the document with the largest or smallest "sort"
     * value.
     */
    top_metrics: {
        /**
         * Selects the fields of the "top" document to return. You can request a
         * single metric with something like "metric": {"field": "m"} or
         * multiple metrics by requesting a list of metrics like "metric": 
         * [{"field": "m"}, {"field": "i"}.
         */
        metric: MetricAggregationConfig<Dataset, Fields> 
            | MetricAggregationConfig<Dataset, Fields>[];

        /**
         * Allows you to add one or more sorts on specific fields. Each sort can
         * be reversed as well. The sort is defined on a per field level, with
         * special field name for _score to sort by score, and _doc to sort by
         * index order.
         */
        sort: SortOptions<Dataset>;

        /**
         * The maximum number of top matching hits to return per bucket. By
         * default the top three matching hits are returned.
         */
        size?: integer;
    }

    /**
     * Counts the number of values that are extracted from the aggregated
     * documents.
     */
    value_count: MetricAggregationConfig<Dataset, Fields>;

    /**
     * Computes the weighted average of numeric values that are extracted from
     * the aggregated documents.
     */
    weighted_avg: {
        /**
         * The configuration for the field or script that provides the values
         */
        value: MetricAggregationConfig<Dataset, Fields>;

        /**
         * The configuration for the field or script that provides the weights
         */
        weight: MetricAggregationConfig<Dataset, Fields>;

        /**
         * The numeric response formatter
         */
        format?: any; // TODO figure out data type

        /**
         * A hint about the values for pure scripts or unmapped fields
         */
        value_type?: any; // TODO figure out data type
    };
}

export type BucketAggregations<Dataset, Fields extends keyof Dataset> = {
    /**
     * A bucket aggregation returning a form of adjacency matrix. The request
     * provides a collection of named filter expressions, similar to the filters
     * aggregation request.
     */
    adjacency_matrix: {
        /**
         * Named list of Elasticsearch filters: { name1: {...}, ...}
         */
        filters: Record<string, {
            terms: Record<Fields,string[]>;
        }>;
        /**
         * Separator character used in response.
         */
        separator: string;
    };

    /**
     * A special single bucket aggregation that selects child documents that
     * have the specified type, as defined in a join field.
     */
    children: {
		/**
		 * Points to type / mapping with the value as name
		 */
        type: string;
    };

    /**
     * A multi-bucket aggregation that creates composite buckets from different
     * sources.
     */
    composite: {
        /**
         * The sources parameter controls the sources that should be used to build
         * the composite buckets. The order that the sources are defined is important
         * because it also controls the order the keys are returned.
         */
        sources: Record<string, {
            terms: MetricAggregationConfig<Dataset, Fields> & {
				/**
				 * By default, the terms aggregation will return the buckets for the top
				 * ten terms ordered by the doc_count. One can change this default
				 * behaviour by setting the size parameter.
				 */
				size: integer;
			};
        }>[];

        /**
         * Can be a string ('asc', 'desc')
         */
        order?: AggSortOptions; // TODO incomplete declaration
        /**
         * The size parameter can be set to define how many composite buckets 
         * should be returned. Used for pagination.
         */
        size?: integer;
        /**
         * To get the next set of buckets, resend the same aggregation with the
         * after parameter set to the after_key value returned in the response. 
         * This request uses the after_key value provided in the previous response.
         */
         after?: {}
    };

    /**
     * This multi-bucket aggregation is similar to the normal histogram, but it
     * can only be used with date values. Because dates are represented
     * internally in Elasticsearch as long values, it is possible, but not as
     * accurate, to use the normal histogram on dates as well.
     */
    date_histogram: MetricAggregationConfig<Dataset, Fields> & {
        /**
         * Specify a single time unit, such as 1 hour or 1 day (a calendar
         * interval), or multiples, such as 6 hours or 3 days (fixed-length
         * intervals).
         */
        interval: DateInterval;  // ES < 7
        // calendar_interval is for ES >= 7
        // calendar_interval: '1m' | '1h' | '1d' | '1w' | '1M' | '1q' | '1y'; 
        // fixed_interval is for ES >= 7 and also needs more thought
        // fixed_interval: '1m' | '1h' | '1d' | '1w' | '1M' | '1q' | '1y';
        /**
         * Use the time_zone parameter to indicate that bucketing should use 
         * a different time zone.
         */
        time_zone: string;
        /**
         * The order of the buckets can be customized by setting the order 
         * parameter. 
         */
        order?: AggSortOptions;
        /**
         * It is possible to only return terms that match more than a 
         * configured number of hits using the min_doc_count option.
         */
        min_doc_count?: integer;
    };

    /**
     * A range aggregation that is dedicated for date values. The main
     * difference between this aggregation and the normal range aggregation is
     * that the from and to values can be expressed in Date Math expressions,
     * and it is also possible to specify a date format by which the from and to
     * response fields will be returned.
     */
    date_range: MetricAggregationConfig<Dataset, Fields> & {
        /**
         * Date Format/Pattern
         */
        format: string;
        /**
         * Array of objects of shape {from: string, to: string} where the
         * strings represent dates.
         */
        ranges: {
            key?: string;
            from?: string;
            to?: string;
        }[];
        /**
         * Setting the keyed flag to true will associate a unique string 
         * key with each bucket and return the ranges as a hash rather than 
         * an array.
         */
        keyed?: boolean;
    };

    /**
     * Like the sampler aggregation this is a filtering aggregation used to
     * limit any sub aggregations' processing to a sample of the top-scoring
     * documents. The diversified_sampler aggregation adds the ability to limit
     * the number of matches that share a common value such as an "author".
     */
    diversified_sampler: MetricAggregationConfig<Dataset, Fields> & {
        /**
         * The optional execution_hint setting can influence the management of
         * the values used for de-duplication. Each option will hold up to
         * `shard_size` values in memory while performing de-duplication but the
         * type of value held can be controlled as follows:
         * 
         *   * hold field values directly (`map`)
         *   * hold ordinals of the field as determined by the Lucene 
         *     index (`global_ordinals`)
         *   * hold hashes of the field values - with potential for hash
         *     collisions (`bytes_hash`)
         */
        execution_hint?: 'map' | 'global_ordinals' | 'bytes_hash';
        /**
         * The max_docs_per_value is an optional parameter and limits how many
         * documents are permitted per choice of de-duplicating value. The
         * default setting is "1".
         */
        max_docs_per_value?: integer;
        /**
         * The shard_size parameter limits how many top-scoring documents are
         * collected in the sample processed on each shard. The default value is
         * 100.
         */
        shard_size?: integer;
    };

    /**
     * Defines a single bucket of all the documents in the current document set
     * context that match a specified filter. Often this will be used to narrow
     * down the current aggregation context to a specific set of documents.
     */
    filter: {
        /**
         * Elasticsearch filter object format
         */
        filter: {}
    };

    /**
     * Defines a multi bucket aggregation where each bucket is associated with a
     * filter. Each bucket will collect all documents that match its associated
     * filter.
     */
    filters: {
        /**
         * can be set to add a bucket to the response which will contain all
         * documents that do not match any of the given filters.
         */
        other_bucket?: boolean;

        /**
         * can be used to set the key for the other bucket to a value other than
         * the default _other_. Setting this parameter will implicitly set the
         * other_bucket parameter to true.
         */
        other_bucket_key?: string;
        /**
         * Named list of Elasticsearch filters: { name1: {...}, ...}
         */
        filters: {}[];
    }

    /**
     * A multi-bucket aggregation that works on geo_point fields and
     * conceptually works very similar to the range aggregation. The user can
     * define a point of origin and a set of distance range buckets.
     */
    geo_distance: MetricAggregationConfig<Dataset, Fields> & {
        /**
         * The `origin` point can accept all formats supported by the 
         * `geo_point` type.
         */
        origin: string;
        /**
         * By default, the distance unit is m (meters) but it can also accept: 
         * mi (miles), in (inches), yd (yards), km (kilometers), 
         * cm (centimeters), mm (millimeters).
         */
        unit: 'm' | 'mi' | 'in' | 'yd' | 'km' | 'cm' | 'mm';
        /**
         * Array of objects of shape {from: ..., to: ...}
         */
        ranges: {}[];
        /**
         * By default, the buckets are returned as an ordered array. It is also
         * possible to request the response as a hash instead keyed by the
         * buckets keys.
         */
        keyed?: boolean;
        /**
         * The distance calculation type can be set using the distance_type
         * parameter.
         */
        distance_type: 'arc' | 'plane';
    }

    /**
     * A multi-bucket aggregation that works on geo_point fields and groups
     * points into buckets that represent cells in a grid. The resulting grid
     * can be sparse and only contains cells that have matching data.
     */
    geohash_grid: {
        /**
         * The name of the field indexed with GeoPoints.
         */
        field: Fields;

        /**
         * The string length of the geohashes used to define cells/buckets in
         * the results. Defaults to 5. 
         */
        precision?: integer;

        /**
         * The maximum number of geohash buckets to return (defaults to 10,000).
         */
        size?: integer;

        /**
         * To allow for more accurate counting of the top cells returned in the
         * final result the aggregation defaults to returning max(10,(size x
         * number-of-shards)) buckets from each shard. If this heuristic is
         * undesirable, the number considered from each shard can be over-ridden
         * using this parameter.
         */
        shard_size?: integer;
    }

    /**
     * Defines a single bucket of all the documents within the search execution
     * context. This context is defined by the indices and the document types
     * youâ€™re searching on, but is not influenced by the search query itself.
     */
    global: {};

    /**
     * A multi-bucket values source based aggregation that can be applied on
     * numeric values extracted from the documents.
     */
    histogram: MetricAggregationConfig<Dataset, Fields> & {
        /**
         * When the aggregation executes, the selected field of every document 
         * will be evaluated and will be rounded down to its closest bucket. 
         * Must be a positive decimal.
         */
        interval: Dataset[Fields];

        /**
         * Shifts bucket boundaries. Must be a decimal greater than or equal to
         * 0 and less than interval.
         */
        offset?: Dataset[Fields];

        /**
         * By default the response will fill gaps in the histogram with empty
         * buckets. It is possible change that and request buckets with a higher
         * minimum count thanks to the min_doc_count setting.
         */
        min_doc_count?: integer;

        /**
         * By default, the buckets are returned as an ordered array. It is also
         * possible to request the response as a hash instead keyed by the
         * buckets keys.
         */
        keyed?: boolean;

        /**
         * The order of the buckets can be customized by setting the order 
         * parameter. 
         */
        order?: AggSortOptions;

        /**
         * With extended_bounds setting, you now can "force" the histogram 
         * aggregation to start building buckets on a specific min value and 
         * also keep on building buckets up to a max value (even if there are 
         * no documents anymore). Using extended_bounds only makes sense when 
         * min_doc_count is 0 (the empty buckets will never be returned if 
         * min_doc_count is greater than 0).
         */
        extended_bounds?: MinMax<Dataset[Fields]>;
    };

    /**
     * A dedicated range aggregation for IP typed fields.
     */
    ip_ranges: MetricAggregationConfig<Dataset, Fields> & {

    }

    /**
     * A field data based single bucket aggregation, that creates a bucket of
     * all documents in the current document set context that are missing a
     * field value (effectively, missing a field or having the configured NULL
     * value set).
     */
    missing:  MetricField<Dataset, Fields>;

    /**
     * A special single bucket aggregation that enables aggregating nested
     * documents.
     */
    nested: {
        /**
         * Path of the nested documents within the top level documents.
         */
        path: string;
    }

    /**
     * A multi-bucket value source based aggregation that enables the user to
     * define a set of ranges - each representing a bucket.
     */
    range: MetricAggregationConfig<Dataset, Fields> & {
        ranges: {

        }[];
    }

    /**
     * A special single bucket aggregation that enables aggregating on parent
     * docs from nested documents. Effectively this aggregation can break out of
     * the nested block structure and link to other nested structures or the
     * root document, which allows nesting other aggregations that arenâ€™t part
     * of the nested object in a nested aggregation.
     */
    reverse_nested: {
        /**
         * Path of the nested documents within the top level documents.
         */
        path?: string;
    }

    /**
     * A filtering aggregation used to limit any sub aggregations' processing to
     * a sample of the top-scoring documents.
     */
    sampler: {
        /**
         * The shard_size parameter limits how many top-scoring documents are
         * collected in the sample processed on each shard. The default value is
         * 100.
         */
        shard_size?: integer;
    }

    /**
     * An aggregation that returns interesting or unusual occurrences of terms
     * in a set.
     */
    significant_terms: MetricAggregationConfig<Dataset, Fields> & {
        /**
         * It is possible to only return terms that match more than a 
         * configured number of hits using the min_doc_count option.
         */
        min_doc_count?: integer;
        /**
         * The default source of statistical information for background term
         * frequencies is the entire index and this scope can be narrowed
         * through the use of a background_filter to focus in on significant
         * terms within a narrower context.
         */
        background_filter?: {
            term: {
                content: string;
            };
        };
        /**
         * The JLH score can be used as a significance score by adding `{}`
         */
        jlh?: Existence;
        /**
         * Mutual information as described in "Information Retrieval", Manning et al.,
         * Chapter 13.5.1 can be used as significance score by adding `{}`
         */
        mutual_information?: {
            include_negatives?: boolean;
            background_is_superset?: boolean;
        };
        /**
         * Chi square as described in "Information Retrieval", Manning et al., Chapter
         * 13.5.2 can be used as significance score by adding `{}`
         */
        chi_square?: Existence;
        /**
         * Google normalized distance as described in "The Google Similarity Distance",
         * Cilibrasi and Vitanyi, 2007 (http://arxiv.org/pdf/cs/0412098v3.pdf) can be
         * used as significance score by adding `{}`
         */
        gnd?: Existence;
        /**
         * A simple calculation of the number of documents in the foreground sample with
         * a term divided by the number of documents in the background with the term.
         * By default this produces a score greater than zero and less than one.
         */
        percentage?: Existence;
    };

    /**
     * An aggregation that returns interesting or unusual occurrences of
     * free-text terms in a set.
     */
    significant_text: MetricAggregationConfig<Dataset, Fields> & {
        /**
         * Filtering near-duplicate text is a difficult task at index-time but
         * we can cleanse the data on-the-fly at query time using the
         * `filter_duplicate_text` setting.
         */
        filter_duplicate_text?: boolean;

        /**
         * The default source of statistical information for background term
         * frequencies is the entire index and this scope can be narrowed
         * through the use of a background_filter to focus in on significant
         * terms within a narrower context.
         */
        background_filter?: {
            term: {
                content: string;
            };
        };

        /**
         * List of JSON _source fields from which text will be analyzed.
         */
        source_fields?: string[];
    }

    /**
     * A multi-bucket value source based aggregation where buckets are 
     * dynamically built - one per unique value.
     */
    terms: MetricAggregationConfig<Dataset, Fields> & {
        /**
         * By default, the terms aggregation will return the buckets for the top
         * ten terms ordered by the doc_count. One can change this default
         * behaviour by setting the size parameter.
         */
        size: integer;
        /**
         * Deferring calculation of child aggregations.
         */
        collect_mode: 'depth_first' | 'breadth_first';
        /**
         * Mechanisms by which terms aggregations can be executed.
         */
        execution_hint: 'global_ordinals' | 'map';
        /**
         * The order of the buckets can be customized by setting the order 
         * parameter. 
         */
        order?: AggSortOptions;
    };
}

export type MatrixAggregations<Dataset, Fields extends keyof Dataset> = {
    matrix_stats: {
        fields: Fields[]
        missing?: {
            [K in Fields]: Dataset[K];
        }
    }
}

export type Aggregations<Dataset, Fields extends keyof Dataset> =
    MetricAggregations<Dataset, Fields> &
    BucketAggregations<Dataset, Fields> /*& 
    MatrixAggregations<Dataset, Fields>*/;

type Aggregation<Dataset, DatasetField extends keyof Dataset> =
	ExactlyOne<Aggregations<Dataset, DatasetField>>;

type Aggs<Dataset, DatasetField extends keyof Dataset> = {
    [aggName: string]: Aggregation<Dataset, DatasetField> & {
        aggs?: Aggs<Dataset, DatasetField>;
    };
};
